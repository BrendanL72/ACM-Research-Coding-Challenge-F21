# -*- coding: utf-8 -*-
"""Sentiment Analysis RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AnNkdO2FoCnYrQBkMk9TJUHyMAc4mrJF

This code was shamelessly ripped/adapted from the "Text classification with an RNN" tutorial offered by the TensorFlow website.

https://www.tensorflow.org/text/tutorials/text_classification_rnn

#Setup

The IMDB reviews are part of the imported TensorFlow Datasets library. As such it does not need to be downloaded from the internet and can be loaded by name.
"""

import numpy as np

import tensorflow_datasets as tfds
import tensorflow as tf

import matplotlib.pyplot as plt


def plot_graphs(history, metric):
  plt.plot(history.history[metric])
  plt.plot(history.history['val_'+metric], '')
  plt.xlabel("Epochs")
  plt.ylabel(metric)
  plt.legend([metric, 'val_'+metric])

dataset, info = tfds.load('imdb_reviews', 
                          with_info =True,
                          as_supervised = True) #VERY IMPORTANT, dataset contains unsupervised data

train_dataset, test_dataset = dataset['train'], dataset['test']

train_dataset.element_spec

for example, label in train_dataset.take(1):
  print('Text: ', example.numpy())
  print('Rating: ', label.numpy())

"""Prefetching some random data from the training and testing datasets to make sure everything works"""

#Constants to determine the amount of data we are prefetching
BUFFER_SIZE = 10000
BATCH_SIZE = 64

train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

num_loops = 1
num_reviews_show = 4
for example, label in train_dataset.take(num_loops):
  print('TEXTS: ', example.numpy()[:num_reviews_show])
  print()
  print('RATINGS: ', label.numpy()[:num_reviews_show])

"""#Creating the text encoder

Notes:
*   Too few allowed vocabulary causes more of the tokens to be unknown ([UNK]) but also run slower 

*   keras.layers.experimental.preprocessing.TextVectorization() processes the text into an array of processed, lowercase tokens that is then turned into vectors. It is a layer. There is a variation that does not preprocess.

*   The adapt() method can be used on a layer to look at the dataset and creates a vocabulary based on the frequency of the words
"""

MAX_VOCAB = 1000 #sets the max number of words the computer can know

encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(
    max_tokens=MAX_VOCAB)

encoder.adapt(train_dataset.map(lambda text, label: text))

vocab = np.array(encoder.get_vocabulary())
#see the most used words, dtype not included
vocab[:30]

"""Testing to see if the numberized token array exists."""

encoded_example = encoder(example)[:3].numpy()
encoded_example

"""Compares the original text with what the computer reads using its limited vocabulary"""

for n in range(3):
  print("Original: ", example[n].numpy())
  print("Round-trip: ", " ".join(vocab[encoded_example[n]]))
  print()

#the explanation for this had a lot of words that i don't understand, look up later
#Sequential allows you to easily lasagna your layers in one line of code
model = tf.keras.Sequential([
    encoder,
    tf.keras.layers.Embedding(
        input_dim=len(encoder.get_vocabulary()), #sets the length of the array
        output_dim=64,    #makes each token/number in the array into a vector of 64 numbers
        mask_zero=True),  #Does padding and masking to handle variable lengths and ignore the padding respectively
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
])

"""Testing the masking. The same text should produce the same results regardless of whether there is padding."""

#no padding
sample_text = ('The movie was cool. The animation and the graphics '
               'were out of this world. I would recommend this movie.')
predictions = model.predict(np.array([sample_text]))
print(predictions[0])

#with padding
padding = "wow " * 2000 #can interchange string with anything to produce same results
predictions = model.predict(np.array([sample_text, padding]))
print(predictions[0])

#compiling the model
model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['accuracy'])

"""#Training the model"""

history = model.fit(train_dataset, epochs=10,
                    validation_data=test_dataset,
                    validation_steps=30)

"""#Testing the model"""

test_loss, test_acc = model.evaluate(test_dataset)

print('Test Loss:', test_loss)
print('Test Accuracy:', test_acc)

plt.figure(figsize=(16, 8))
plt.subplot(1, 2, 1)
plot_graphs(history, 'accuracy')
plt.ylim(None, 1)
plt.subplot(1, 2, 2)
plot_graphs(history, 'loss')
plt.ylim(0, None)

"""#Testing the code on the given text"""

